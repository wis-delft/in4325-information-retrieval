{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTerrier\n",
    "\n",
    "_DSAIT4050: Information retrieval lecture, TU Delft_\n",
    "\n",
    "**Part 7: Neural ranking models**\n",
    "\n",
    "In this part, we'll learn how to use **neural ranking models** with PyTerrier. Since PyTerrier itself does not implement any neural rankers, we'll use the [**Fast-Forward indexes** library](https://github.com/mrjleo/fast-forward-indexes), which focuses on efficient re-ranking using neural models and provides PyTerrier integration (using transformers). In order to learn more about Fast-Forward indexes, have a look at the [documentation](https://mrjleo.github.io/fast-forward-indexes/docs/v0.7.0/) and the [corresponding paper](https://dl.acm.org/doi/abs/10.1145/3485447.3511955).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install python-terrier==0.12.1 fast-forward-indexes==0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: To execute this notebook in reasonable time, you'll need a CUDA-capable GPU. If you have one, follow the [official tutorials](https://pytorch.org/get-started/locally/) and install PyTorch with CUDA acceleration. If you do not have one, you can use Google Colab: Under \"Edit\" -> \"Notebook settings\" -> \"Hardware accelerator\", select a GPU.\n",
    "\n",
    "If the installation was successful, the following should return `True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Side note_: In this notebook, we focus on the **retrieve-and-re-rank** setting. PyTerrier supports **dense retrieval** models through [plugins](https://pyterrier.readthedocs.io/en/latest/ext/pyterrier-dr/index.html). Another library that provides many pre-trained models and dense retrieval indexes is [pyserini](https://github.com/castorini/pyserini).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the [FiQA dataset](https://sites.google.com/view/fiqa/home), which is a dataset for financial question answering. Since QA pipelines usually include a retrieval/ranking step, FiQA provides a corpus, queries (questions), and corresponding QRels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"irds:beir/fiqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the domain of this dataset (finance), the questions and documents are relatively complex. We'll see how much a large neural re-ranking model (based on BERT) manages to improve over term matching (BM25).\n",
    "\n",
    "Let's create a lexical index first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "indexer = pt.IterDictIndexer(\n",
    "    str(Path.cwd()),  # this will be ignored\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    ")\n",
    "index_ref = indexer.index(dataset.get_corpus_iter(), fields=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our baseline, we measure BM25 performance without any re-ranking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import RR, nDCG, MAP\n",
    "\n",
    "bm25 = pt.terrier.Retriever(index_ref, wmodel=\"BM25\")\n",
    "testset = pt.get_dataset(\"irds:beir/fiqa/test\")\n",
    "pt.Experiment(\n",
    "    [bm25],\n",
    "    testset.get_topics(),\n",
    "    testset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 10, MAP @ 100],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-Forward indexes\n",
    "\n",
    "Fast-forward indexes use _dual-encoder models_ (the same that are used in dense retrieval) for _interpolation-based re-ranking_. The benefit of this (compared to cross-encoders) is that document representations only need to be computed once (during the indexing step) and can be looked up during re-ranking.\n",
    "\n",
    "### The encoders\n",
    "\n",
    "We'll start by instantiating the encoders. [TAS-B](https://github.com/sebastian-hofstaetter/tas-balanced-dense-retrieval) is a single-vector dual-encoder model based on BERT, where the query and document encoders are identical (Siamese architecure). A pre-trained model (trained on MS MARCO) is [available on the Hugging Face hub](https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco). We'll use this model in a transfer setting (i.e., without fine-tuning) on the FiQA dataset.\n",
    "\n",
    "The encoders can be loaded as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.encoder import TASBEncoder\n",
    "import torch\n",
    "\n",
    "q_encoder = d_encoder = TASBEncoder(\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these encoders to compute vector representations of text pieces, such as queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_encoder([\"query 1\", \"query 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The index\n",
    "\n",
    "We've already created an index for the BM25 retriever earlier. For the dense vector representations, we'll need to create another separate index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.index import OnDiskIndex, Mode\n",
    "\n",
    "ff_index_path = Path.cwd() / \"07_data\" / \"ffindex_fiqa_tasb.h5\"\n",
    "ff_index_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "ff_index = OnDiskIndex(\n",
    "    ff_index_path,\n",
    "    query_encoder=q_encoder,\n",
    "    mode=Mode.MAXP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mode.MAXP` determines how documents that have multiple vectors are scored; however, since our documents only have a single representation each, this will have no effect; we just need to set this to tell the index we're working with documents and not passages (more on that [here](https://mrjleo.github.io/fast-forward-indexes/docs/v0.7.0/fast_forward/index.html#ranking-mode)).\n",
    "\n",
    "Now we can index the corpus using our document encoder. This is done using the `Indexer` utility class. We'll use an iterator that simply yields the documents in the correct format.\n",
    "\n",
    "**The next cell will take a while to execute, even with GPU acceleration.** You can adjust the batch size according to your available VRAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.util import Indexer\n",
    "\n",
    "\n",
    "def docs_iter():\n",
    "    for d in dataset.get_corpus_iter():\n",
    "        yield {\"doc_id\": d[\"docno\"], \"text\": d[\"text\"]}\n",
    "\n",
    "\n",
    "Indexer(ff_index, d_encoder, batch_size=8).from_dicts(docs_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once indexing is done, we can always load the index instead of indexing everything again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.index import OnDiskIndex, Mode\n",
    "\n",
    "ff_index = OnDiskIndex.load(\n",
    "    Path.cwd() / \"07_data\" / \"ffindex_fiqa_tasb.h5\",\n",
    "    query_encoder=q_encoder,\n",
    "    mode=Mode.MAXP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, if you have enough RAM, you can load the entire index (i.e., all vector representations) into the main memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_index = ff_index.to_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking BM25 results\n",
    "\n",
    "We've already performed re-ranking in the learning-to-rank notebook. In order to use a Fast-Forward index for re-ranking, we wrap it in an `FFScore` transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.util.pyterrier import FFScore\n",
    "\n",
    "ff_score = FFScore(ff_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how this works, we take the queries from the testset and retrieve a small number of candidate documents for each one using BM25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = (bm25 % 5)(testset.get_topics())\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, this data frame contains a score for each query-document pair. We can apply our new re-ranking transformer to the candidates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranked = ff_score(candidates)\n",
    "re_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `score` column has now been updated to reflect the re-ranking scores. Furthermore, there is a new column, `score_0`, which contains the original retrieval scores. As mentioned earlier, Fast-Forward indexes focus on _interpolation-based re-ranking_. In essence, the idea is to take both lexical retrieval scores $s_{\\text{lex}}$ and semantic re-ranking scores $s_{\\text{sem}}$ into account, such that the final score $s$ is computed as follows:\n",
    "\n",
    "$$s = \\alpha s_{\\text{lex}} + (1-\\alpha) s_{\\text{sem}}$$\n",
    "\n",
    "We can perform the interpolation using the `FFInterpolate` transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_forward.util.pyterrier import FFInterpolate\n",
    "\n",
    "ff_int = FFInterpolate(alpha=0.5)\n",
    "ff_int(re_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data frame above, both scores have been fused into one with equal weights.\n",
    "\n",
    "Now we're ready to take everything for a spin. Let's compare our re-ranker to BM25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    [bm25, bm25 % 1000 >> ff_score >> ff_int],\n",
    "    testset.get_topics(),\n",
    "    testset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 10, MAP @ 100],\n",
    "    names=[\"BM25\", \"BM25 >> FF\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the performance already improved quite nicely. But thinking back, we chose $\\alpha=0.5$ pretty much arbitrarily. How do we know this is really the best value?\n",
    "\n",
    "### Validation\n",
    "\n",
    "PyTerrier offers several functions to determine the best hyperparameters for a ranker. In the following, we'll use [`pyterrier.GridSearch`](https://pyterrier.readthedocs.io/en/latest/tuning.html#pyterrier.GridSearch) to find the best value for $\\alpha$.\n",
    "\n",
    "**Important**: When you tune hyperparameters of your model, **do not use the same data you use for testing (i.e., the testset)**. Otherwise, your results are invalid, because you optimized your method for the testing data. Instead, we'll use the development (validation) data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = pt.get_dataset(\"irds:beir/fiqa/dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTerriers `GridSearch` class can be used to automatically run an experiment multiple times in order to find the hyperparameters that result in the best performance.\n",
    "\n",
    "We'll use a similar pipeline as before, but we limit the number of candidate documents to `100` in order to reduce the runtime. We provide a list of values for `alpha` and a metric (MAP), which is used to decide which value results in the best performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.GridSearch(\n",
    "    bm25 % 100 >> ff_score >> ff_int,\n",
    "    {ff_int: {\"alpha\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}},\n",
    "    devset.get_topics(),\n",
    "    devset.get_qrels(),\n",
    "    \"map\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, out of the options we provided, `alpha=0.3` was the best.\n",
    "\n",
    "Conveniently, the best value has already been set for us in the transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_int.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat our experiment on the testset with the optimal hyperparameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    [bm25, bm25 % 1000 >> ff_score >> ff_int],\n",
    "    testset.get_topics(),\n",
    "    testset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 10, MAP @ 100],\n",
    "    names=[\"BM25\", \"BM25 >> FF\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the value of `alpha` makes a big difference!\n",
    "\n",
    "_Final remarks_: We've used neural models for re-ranking only in this notebook. However, in practice, it is not uncommon to use the re-ranking scores we computed here as a feature for a learning-to-rank model.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "Check out the [section on neural models](https://pyterrier.readthedocs.io/en/latest/neural.html) in the PyTerrier documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
