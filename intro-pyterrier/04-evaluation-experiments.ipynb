{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTerrier\n",
    "\n",
    "_DSAIT4050: Information retrieval lecture, TU Delft_\n",
    "\n",
    "**Part 4: Evaluation & experiments**\n",
    "\n",
    "This part focuses on running experiments and evaluating retrieval and ranking models using PyTerrier. We'll learn how to use query relevances (QRels) to compute ranking metrics and perform statistical significance testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install python-terrier==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries and query relevance\n",
    "\n",
    "Let's start by loading an existing dataset. We'll pick one from [the list](https://pyterrier.readthedocs.io/en/latest/datasets.html#available-datasets) that has topics (queries) and QRels available, such as `vaswani`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"vaswani\")\n",
    "index = dataset.get_index(variant=\"terrier_stemmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the queries (topics) in this dataset. Note that this dataset only has one set of queries and QRels (you can see this in the table). If a dataset has multiple sets, you'll need to specify one, for example: `dataset.get_topics(\"test\")`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the set of queries, we can inspect the corresponding relevance data (QRels). These are tuples of (query, document, label). Note that labels can be either binary (`0` or `1`) or graded (for example, `0` to `3`, where a higher number encodes higher relevance). Tuples where the label is `0` (i.e., no relevance) are often omitted from the QRels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_qrels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Using the QRels and the output of a ranking model (i.e., a _ranking_ or a _run_), we can compute ranking metrics that evaluate the quality of our results. Let's start with a BM25 model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An _experiment_ in PyTerrier automates the retrieval/ranking and evaluation process. This means that you can get a comprehensive evaluation of models or systems on some test set in a single line of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import RR, nDCG, MAP\n",
    "\n",
    "pt.Experiment(\n",
    "    [bm25],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have evaluted our BM25 retriever on the dataset we loaded earlier. We used three of the most common IR metrics, but [PyTerrier supports a very large number of additional metrics](https://pyterrier.readthedocs.io/en/latest/experiments.html#evaluation-measures-objects) through its integration of the [ir-measures](https://github.com/terrierteam/ir_measures) library.\n",
    "\n",
    "**Important**: Usually, a value of `0` indicates _no relevance_, while a value greater than `0` indicates _relevance_. However, this is not always the case; a popular example is the TREC Deep Learning passage ranking task, where [only values greater than `1` indicate relevance](https://trec.nist.gov/data/deep2020.html). In those cases, the threshold can by adjusted by constructing the metric as, for example, `RR(rel=2)`.\n",
    "\n",
    "Note that we used the `@` operator to limit the retrieval depth (number of documents per query) for specific metrics. Furthermore, since both queries (topics) and QRels are simply packaged in a `pandas.DataFrame`, you can easily provide your own data, as long as it is in the right format.\n",
    "\n",
    "### Performance comparisons\n",
    "\n",
    "Experiments let you easily compare multiple ranking approaches. Let's load another ranker, which uses standard TF-IDF, to see how much BM25 improves over it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pt.terrier.Retriever(index, wmodel=\"TF_IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run an experiment that evaluates both models on the same data. For better readability, we assign custom names to both approaches.\n",
    "\n",
    "Comparing the approaches in the same experiment allows us to automatically have _statistical significance testing_ performed. By setting `baseline=0`, we tell the function to compute the $p$-values with respect to the first approach (TF-IDF). Furthermore, [PyTerrier supports a number of correction methods](https://pyterrier.readthedocs.io/en/latest/experiments.html#significance-testing); here, we apply Bonferroni correction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    [tf_idf, bm25],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"TF-IDF\", \"BM25\"],\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    "    baseline=0,\n",
    "    correction=\"bonferroni\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results\n",
    "\n",
    "Experiments allow for saving the obtained results (runs) in the common TREC format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path.cwd() / \"04_data\" / \"results\"\n",
    "results_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "pt.Experiment(\n",
    "    [tf_idf, bm25],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"TF-IDF\", \"BM25\"],\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    "    save_dir=str(results_dir),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful when results need to be shared or processed futher. In addition, PyTerrier will re-use saved results rather than re-computing them. This behavior can be disabled by setting `save_mode=\"overwrite\"`.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "Check out the [section on experiments](https://pyterrier.readthedocs.io/en/latest/experiments.html) in the documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
